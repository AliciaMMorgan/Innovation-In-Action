# AI WINS Dashboard: From Strategy to Execution

## Origin and Context

The AI WINS dashboard was developed as a practical output of the **MasterClass “AI Strategy at Work: How to Become Indispensable”** and then applied to the Innovation‑In‑Action PM Risk Assessor.  
It operationalizes the MasterClass automate / augment / humanize framing for a **Senior Program Leader – AI Execution** role, with a focus on government IT, PMBOK 8, and NIST AI RMF–aligned delivery.

---

## Purpose

The AIWINS dashboard helps leaders move from abstract AI strategy to **executable, defensible decisions**. Its goal is to make AI adoption visible, structured, and grounded in business value rather than hype or experimentation for its own sake.

The dashboard provides a clear way to assess:

- Which tasks can be **automated**
- Which tasks should be **augmented** by AI
- Which tasks must remain **human-led**
- Where **trust, governance, and risk** considerations shape deployment

This work directly addresses the execution gap between AI awareness and real operational implementation.

---

## Problem Statement

Most organizations understand *that* AI matters, but struggle to answer:

- Where should AI be applied first?
- How do we avoid over‑automation?
- How do we justify AI decisions to skeptical stakeholders?
- How do we keep humans meaningfully in the loop?

AI pilots often stall because decisions are made without a **repeatable framework** that balances efficiency, risk, and human judgment.

The AIWINS dashboard was designed to solve this problem by turning strategy into a **decision‑ready artifact**.

---

## Approach Overview

The dashboard was built using a structured, **task‑level analysis** rather than role‑level or technology‑first thinking.

The process focused on:

1. Breaking work down into discrete tasks  
2. Evaluating each task against consistent decision criteria  
3. Visualizing outcomes in an executive‑readable format

This ensures the output can support leadership conversations, governance reviews, and workforce planning.

---

## Methodology

### 1. Task Decomposition

Work was decomposed into individual tasks rather than job titles or departments. This avoided over‑generalization and enabled more precise decisions.

Each task was documented with:

- Core objective  
- Inputs and outputs  
- Level of judgment required  
- Risk and consequence of error  
- Dependency on human context or relationships  

### 2. AI Suitability Assessment

Each task was evaluated across three possible paths:

- **Automate**  
  Repeatable, rules‑based, low‑risk, outcome‑consistent tasks.

- **Augment**  
  Tasks where AI improves speed, analysis, or synthesis, while human judgment remains essential.

- **Human‑Led**  
  Tasks requiring contextual understanding, ethical judgment, accountability, or trust‑based decision‑making.

This assessment emphasizes **appropriateness**, not maximum automation.

### 3. Trust and Risk Considerations

Beyond technical feasibility, each task was reviewed for:

- Error tolerance  
- Regulatory or compliance implications  
- Reputational risk  
- Stakeholder trust impact  

This ensures the dashboard reflects real‑world constraints, not just theoretical capability.

### 4. Visualization and Synthesis

The results were synthesized into an executive‑ready dashboard that:

- Shows distribution of work across automate, augment, and human‑led categories  
- Highlights high‑risk or high‑impact decision points  
- Supports scenario‑based discussion rather than prescriptive mandates  

The dashboard is designed as a **conversation tool**, not a static report.

---

## Output

The AIWINS dashboard delivers:

- A structured view of AI opportunity grounded in actual work  
- Clear justification for why tasks fall into specific categories  
- A defensible artifact for leadership, governance, and workforce discussions  
- A repeatable framework that can be adapted across industries and functions  

In the Innovation‑In‑Action implementation, it quantifies:

- ~28 hours per week of risk‑related work shifted or streamlined  
- Up to \$3.2M+ potential portfolio impact across government IT programs

---

## Why This Matters

AI fluency is not demonstrated by tool usage alone. It is demonstrated by the ability to:

- Assess **where** AI adds value  
- Know **where** it should not be used  
- Balance efficiency with accountability  
- Design systems that humans can trust  

The AIWINS dashboard **operationalizes** these principles in a practical, reusable way and aligns directly with the MasterClass emphasis on becoming indispensable through strategic AI decision‑making.

---

## Relationship to Innovation‑In‑Action

Within the Innovation‑In‑Action repository, the AIWINS dashboard serves as:

- A concrete example of **AI strategy translated into PM workflows**  
- A model for task‑level AI decision‑making in risk management  
- A foundation for future agent deployment and workflow automation  

It forms the bridge between:

- Strategy and execution  
- AI capability and organizational reality  
- Knowledge and trust  

---

## Files and Usage

- `[artifacts/AI-Wins-Dashboard.pdf]` – Three‑tab view exported from Google Sheets:
  - **Tab 1: AI Wins Dashboard** – Task‑level automate / augment / human‑led view with time saved and quality deltas.
  - **Tab 2: Risk × Strategic Moat** – Matrix of Automation Risk vs Strategic Moat across key PM risk tasks.
  - **Tab 3: Portfolio Rollup** – Single / 5‑program / 15‑program view, including hours saved and \$3.2M+ impact estimates.

*(If you keep an editable sheet in the repo, you can also add:)*  
- artifacts/AI-Wins-Dashboard – Optional source template organizations can adapt to their own portfolios.

**How to use:**

- Plug in your own program or portfolio tasks.  
- Re‑score automate / augment / human‑led decisions.  
- Use the Risk × Moat view to prioritize pilots and avoid over‑automation.

---

## Status and Next Steps

- Dashboard framework completed and linked to the Senior Program Leader – AI Execution role.  
- Initial use cases validated through practitioner feedback.  
- Insights informing Q1 2026 agent and workflow development within Innovation‑In‑Action.  
- Ongoing refinement based on cross‑industry application and emerging NIST / PMBOK guidance.
