# Sample Assessment: STEM Museum Scaling Initiative

## Context

This is a real example of applying the PM Risk Assessor prompts to a museum education program scaling from 30,000 to 45,000+ students annually while maintaining governance and audit readiness.

**Initiative:** Scale STEM education programs across K-12 partnerships  
**Timeline:** 2017-2020  
**Starting Scale:** 30,000 students/year  
**Target:** 5%+ annual growth with audit-ready outcomes reporting  
**Partners:** Multiple school districts, external evaluation vendor, higher education institutions

---

## Prompt 1: Initial Risk Assessment

### Input Summary
- **Scale:** 30,000 students/year across field trips, planetarium, outreach, camps
- **Data Sources:** Survey tools, spreadsheets, partner systems
- **Partners:** School districts, community organizations, evaluation vendor
- **Timeline:** Board mandate for 5% annual growth
- **Success Metrics:** Student reach, STEM knowledge/interest/confidence, teacher PD participation

### Top 5 Risks Identified

**Risk 1: Taxonomy Fragmentation**
- **Category:** Taxonomy & Standards
- **Likelihood:** High
- **Impact:** High
- **Evidence:** Surveys created independently by program staff, no shared instrument design, inconsistent response scales
- **Next Step:** Audit all existing surveys and map to a unified logic model (inputs → activities → outputs → outcomes)

**Risk 2: Cross-District Data Ownership Ambiguity**
- **Category:** Data Ownership & Privacy
- **Likelihood:** High
- **Impact:** High
- **Evidence:** Student/teacher data collected by museum but legally owned by school districts, no formal data sharing agreements, unclear retention responsibilities
- **Next Step:** Draft data sharing MOUs clarifying ownership, privacy controls, and FERPA compliance responsibilities

**Risk 3: Pipeline Tracking Failure**
- **Category:** Pipeline Patterns
- **Likelihood:** High
- **Impact:** Medium
- **Evidence:** Can't track students longitudinally from elementary through high school into workforce, no shared identifiers across partner districts
- **Next Step:** Design feeder pattern framework to map student progression and identify where tracking breaks down

**Risk 4: Missing Audit Documentation**
- **Category:** Audit Readiness
- **Likelihood:** Medium
- **Impact:** High
- **Evidence:** Teacher PD hours tracked informally, no evidence of state compliance for continuing education credits, evaluation methodology not validated externally
- **Next Step:** Inventory audit requirements (state CPE rules, outcomes validation standards) and gap current documentation

**Risk 5: Misaligned Partner Expectations**
- **Category:** Stakeholder Alignment
- **Likelihood:** Medium
- **Impact:** Medium
- **Evidence:** School partners expect immediate test score improvement, museum focuses on long-term STEM pipeline, funders want workforce-ready outcomes, timelines mismatched
- **Next Step:** Facilitate alignment session with partners to establish shared success criteria and realistic timelines

---

## Prompt 2: Data Governance Audit

### Critical Gaps Found

**Taxonomy Issues:**
- Survey questions about "STEM interest" used 3-point scale in some programs, 5-point in others
- "Confidence" measured differently across grade bands
- No data dictionary mapping instruments to logic model outcomes

**Ownership Gaps:**
- Museum staff assumed they owned student data because they collected it
- School districts assumed data belonged to them per FERPA
- External evaluator had unclear rights to retain anonymized datasets
- No documented retention schedules

**Privacy Risks:**
- Student names collected unnecessarily in some contexts
- Partner access to raw data via shared folders without access controls
- No formal process for obtaining parent consent for longitudinal tracking

**Remediation Actions Taken:**
1. Created unified logic model (inputs/activities/outputs/outcomes)
2. Redesigned surveys with standardized questions and response scales
3. Drafted data sharing agreements clarifying FERPA responsibilities
4. Implemented secure data lifecycle: online forms → structured exports → password-protected transfer → external outcomes platform
5. Established seven-year retention for teacher PD records per CPE requirements

---

## Prompt 3: Pipeline Pattern Analysis

### Pipeline Mapped

**Student STEM Pipeline:**
Elementary (ages 5-10) → Middle School (ages 11-13) → High School (ages 14-18) → College/Workforce (ages 18+)

**Organizational Boundaries:**
- Elementary: 15+ school districts
- Middle School: 10+ districts (some consolidation)
- High School: 8+ districts
- College: Regional higher education partners
- Workforce: Industry partners (aviation, engineering)

### Critical Dependencies Identified

**Dependency 1: Cross-District Feeder Patterns**
- **Stage:** Elementary → Middle School
- **Boundary:** Different school districts
- **Risk:** High
- **Failure Mode:** Can't track if elementary STEM exposure influences middle school course selection
- **Gap:** No shared student identifiers, privacy constraints on linking data
- **Recommendation:** Build aggregate cohort tracking by grade/year rather than individual tracking

**Dependency 2: Teacher PD → Student Outcomes**
- **Stage:** Teacher training → Classroom implementation → Student learning
- **Boundary:** Museum → School district → Individual classrooms
- **Risk:** High
- **Failure Mode:** Can't prove PD effectiveness if teacher implementation not tracked
- **Gap:** No follow-up mechanism to observe classroom integration
- **Recommendation:** Add post-PD survey asking teachers how they used museum resources

**Dependency 3: Museum Programs → Workforce Readiness**
- **Stage:** K-12 exposure → College major selection → Aviation/STEM careers
- **Boundary:** Education system → Workforce
- **Risk:** Medium
- **Failure Mode:** Can't demonstrate long-term impact on "day-one ready" workforce talent
- **Gap:** No tracking mechanism for students 5-10 years after programs
- **Recommendation:** Partner with higher education to survey alumni about STEM experiences

### Bottleneck: Limited Capacity for Longitudinal Analysis
- Most programs measured immediate pre/post outcomes only
- No system for tracking participants years later
- Manual processes couldn't scale beyond 30K students
- **Solution:** Implemented external outcomes platform with dashboards by grade/program/year

---

## Prompt 4: Audit Readiness Check

### Audit Scenarios Evaluated

**Scenario 1: State CPE Provider Audit**
- **Requirement:** Demonstrate teacher PD meets state continuing education rules
- **Gaps Found:**
  - No documented facilitator qualifications
  - Attendance tracked in spreadsheets, not audit-ready system
  - Evaluation forms existed but not systematically collected
  - No evidence of seven-year record retention
- **Remediation:**
  - Documented facilitator credentials and approval
  - Implemented sign-in/sign-out system with hour calculations
  - Standardized post-session evaluations
  - Established archival process for attendance records
- **Result:** Achieved state CPE provider approval

**Scenario 2: Outcomes Certification Audit**
- **Requirement:** Demonstrate valid methodology for outcomes measurement
- **Gaps Found:**
  - Evaluation instruments not externally validated
  - Data analysis methods not documented
  - No evidence of program improvements based on data
  - Stakeholder engagement in evaluation not systematic
- **Remediation:**
  - Engaged external evaluation vendor for methodology review
  - Documented survey design aligned to logic model
  - Created feedback loops showing program adjustments based on outcomes
  - Established regular stakeholder review of data dashboards
- **Result:** Achieved regional nonprofit outcomes/data-governance certification

### Audit Readiness Score: 4/10 → 9/10

**Show-Stoppers Addressed:**
- FERPA compliance documentation (data sharing agreements)
- CPE record retention (seven-year archives)
- Outcomes methodology validation (external review)

**Quick Wins Implemented:**
- Data dictionaries for all instruments
- Version control for surveys
- Stakeholder dashboard access

**Long-Term Investments:**
- External outcomes platform ($X/year)
- Evaluation vendor relationship
- Staff training on data governance

---

## Prompt 5: Stakeholder Alignment Diagnostic

### Misalignments Discovered

**Misalignment 1: Definition of "STEM Interest"**
- **Stakeholders:** Museum program staff, school district administrators, external evaluator
- **Type:** Definition
- **Severity:** High
- **Evidence:** Program staff measured "excitement during visit," schools measured "enrollment in STEM courses," evaluator measured "career intent"
- **Resolution:** Created shared logic model with explicit outcome definitions and measurement instruments for each level (knowledge, interest, confidence, intent)

**Misalignment 2: Timeline Expectations**
- **Stakeholders:** Funders, leadership, program managers
- **Type:** Timeline
- **Severity:** High
- **Evidence:** Funders expected year-over-year growth proof, leadership wanted immediate impact stories, program managers knew longitudinal change takes 3-5 years
- **Resolution:** Established tiered reporting: immediate outputs (reach, touchpoints), annual outcomes (pre/post gains), longitudinal trends (cohort tracking)

**Misalignment 3: Data Ownership Assumptions**
- **Stakeholders:** Museum leadership, school district partners, legal counsel
- **Type:** Authority
- **Severity:** Critical
- **Evidence:** Museum assumed data collection meant ownership, districts cited FERPA, no one had documented the actual legal status
- **Resolution:** Legal review confirmed student data owned by districts, museum acts as service provider, formalized in data sharing agreements

**Misalignment 4: Success Metric Priorities**
- **Stakeholders:** Board members, program teams, external partners
- **Type:** Incentive
- **Severity:** Medium
- **Evidence:** Board wanted "total reach" growth, programs cared about "quality outcomes per student," partners needed "workforce pipeline" evidence
- **Resolution:** Dashboards reporting all three metrics by audience, no single "success number"

### Alignment Score: 3/10 → 8/10

**Communication Improvements:**
- Quarterly stakeholder reviews of dashboards
- Explicit success criteria documentation
- Shared glossary of terms

---

## Outcomes: 2017-2020

**Scale Achievements:**
- Student reach: 30,000 → 45,000+ (50% growth, exceeding 5% annual target)
- Revenue: $600K+ in grants secured
- Programs: Expanded mix while maintaining quality

**Governance Validation:**
- State CPE provider approval achieved
- Regional outcomes/data-governance certification achieved
- Audit-ready documentation established

**Capability Building:**
- Unified taxonomy and logic model
- Governed data pipeline to outcomes platform
- Cross-partner alignment on success criteria
- Staff capacity for data-driven decision making

---

## Key Lessons for Future Agents

These patterns informed the PM Risk Assessor design:

1. **Taxonomy failures emerge gradually** - Systems that "work" at small scale break when you try to aggregate or compare
2. **Pipeline dependencies are invisible until tested** - Cross-boundary flows look fine until you try to track end-to-end outcomes
3. **Audit readiness is practical, not theoretical** - External validations (CPE approval, certification) proved the governance system worked
4. **Stakeholder alignment requires explicit documentation** - Assumed agreement masks real conflicts until they block progress
5. **Quick wins enable long-term change** - Data dictionaries and shared glossaries created immediate value while systems developed

---

*This sample demonstrates how the PM Risk Assessor prompts surface execution gaps before they become scaling blockers.*  
*Based on real program leadership experience (VP Education & Programs, 2017-2020)*
